# DeepEncrypt
>Author: Rman

## Background
Now a day, machine learning and deep learning is widely used in many aspects include medical area, finical area and web security area. However, deep learning requires a great mount of data as support.  In that case, how to protect the privacy of the training data is becoming very important. In research area, many methods like [Model Inversion Attacks](https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf)are proposed to steal the training data.

This challenge simulate a deep learning model which "encrypts" some data (flag) to a "encrypted" data (flag_enc). Player should use the training data and the "enc" model to train a "decrypt" model and get the raw flag. 

## Challenge
### FIles：

* flag_sample.txt : flag sample for training
* enc_sample.txt : encrypt flag sample for training
* enc.hdf5: Pre-trained model having used the data above.
* flag_enc.txt:  Encrypted flag for player to decrypt (flag in server-> enc.hdf5-> flag_enc.txt)
### Server：
Player should input the "decrypt" flag to the server and the server will compare the input with the flag in the server.
```python
import numpy as np

flag = np.loadtxt("../data/flag.txt")
true_flag = "de1ctf{xxx_xxx_xxx}"
threshold=0.2

def mse(true, predict):
    loss = np.average(np.abs(true - predict))
    print(loss)
    return loss


def judge(predict):
    if mse(flag, predict) < threshold:
        print(true_flag)
    else:
        print("You can't fool me")


if __name__ == "__main__":
    inp = input("Input your flag_dec result:")
    inp = np.asarray(inp.split(' '), dtype=float)
    judge(inp)
```

## Solve
Using AutoEncoderDecoder to train a decrypt model.

You can just run `python solve.py` and the result will store in flag_dec.txt. Maybe you need to run several times to get the result.

```
requirements

keras
sklearn
numpy
```



Dec model:

|Layer (type)|                 Output Shape|              Param |
|:---:|:---:|:---:|
|input_1 (InputLayer)|         (None, 64)|                0|
|dense_1 (Dense)  |            (None, 2048)|              133120|
|dense_2 (Dense)|              (None, 2048) |             4196352|
|dense_3 (Dense) |             (None, 128)   |            262272  |
Total params: 4,591,744

Trainable params: 4,591,744

Non-trainable params: 0
_________________________________________________________________
AutoEncoderDecoder：

|Layer (type)|                 Output Shape|              Param |
|:---:|:---:|:---:|
|Enc (Model)|                  (None, 64)|                8256 |
|Dec (Model)|                  (None, 128)               |4591744 |
Total params: 4,600,000

Trainable params: 4,591,744

Non-trainable params: 8,256
```python
def dec_model(enc_shape, flag_shape):
    inp = Input((enc_shape,))
    h = Dense(2048)(inp)
    h = Dense(2048)(h)
    out = Dense(flag_shape, activation='sigmoid')(h)
    return Model(inp, out)
def load_data(flag_name, enc_name):
    '''

    :param path: data path
    :return:
        flag_sample: shape=(512,128)
        enc_sample:shape=(512,64)
    '''
    flag_sample = np.loadtxt(flag_name)
    enc_sample = np.loadtxt(enc_name)
    return flag_sample, enc_sample
    
def train_dec(flag_sample, enc_sample):
    flag_shape = flag_sample.shape[-1]
    enc_shape = enc_sample.shape[-1]
    Enc_model = load_model("../model/enc.hdf5")
    Enc_model.name = "Enc"
    Dec_model = dec_model(enc_shape, flag_shape)
    print("Train Dec_model")
    Enc_model.trainable = False
    inp = Enc_model.inputs
    dec = Enc_model(inp)
    out = Dec_model(dec)
    model = Model(inp, out)
    model.compile(loss='mean_absolute_error', optimizer='Adam')
    print(model.summary())
    ear = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)
    model.fit(flag_sample, flag_sample, batch_size=512, epochs=100000000, verbose=2, validation_split=0.1,
              callbacks=[ear])
    print(Dec_model.summary())
    Dec_model.save(dec_loss0.177.hdf5)


def solve():
    Dec_model = load_model(dec_loss0.177.hdf5)
    flag_enc = np.loadtxt("../data/flag_enc.txt").reshape(1, -1)
    flag_dec = Dec_model.predict(flag_enc)
    np.savetxt("../data/flag_dec.txt", flag_dec)
    # print(flag_dec[0])
    judge(flag_dec[0])
```
## Result
loss: 0.17741050019098772

delta{xxx_xxx_xxx}

flag: 
>1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0

flag_enc:
>-4.286013841629028320e-01 9.896190166473388672e-01 4.559664130210876465e-01 8.176887035369873047e-01 8.356271386146545410e-01 3.765194416046142578e-01 1.687297374010086060e-01 3.029667437076568604e-01 5.969925522804260254e-01 5.114848613739013672e-01 9.926454722881317139e-02 9.131879210472106934e-01 -2.152046710252761841e-01 8.866041898727416992e-02 3.317154347896575928e-01 9.851776361465454102e-01 7.276151180267333984e-01 8.283065557479858398e-01 1.823632977902889252e-03 3.699933588504791260e-01 6.979680061340332031e-02 1.828217357397079468e-01 5.757516622543334961e-01 1.914786100387573242e-01 3.244600296020507812e-01 1.111515283584594727e+00 5.159097313880920410e-01 1.231751441955566406e-01 -3.645407259464263916e-01 7.166512608528137207e-01 1.389274299144744873e-01 7.724004983901977539e-02 7.178838849067687988e-01 -9.603453427553176880e-02 5.028448104858398438e-01 3.499638140201568604e-01 8.395515680313110352e-01 6.976196765899658203e-01 2.593761086463928223e-01 7.141951918601989746e-01 6.022385954856872559e-01 1.001740217208862305e+00 -2.897696197032928467e-01 1.448748558759689331e-01 8.408914208412170410e-01 2.470737695693969727e-01 4.430454969406127930e-01 -2.019447684288024902e-01 8.161327838897705078e-01 2.832469642162322998e-01 6.612138748168945312e-01 9.899861216545104980e-01 2.219144105911254883e-01 1.322134375572204590e+00 7.497617006301879883e-01 9.182292222976684570e-01 6.070237755775451660e-01 3.877772092819213867e-01 3.660472482442855835e-02 7.972034811973571777e-01 -2.158393338322639465e-02 5.925227403640747070e-01 5.734952688217163086e-01 -5.487446486949920654e-02

flag_dec:
>9.999969005584716797e-01 1.000000000000000000e+00 1.000000000000000000e+00 8.216343522071838379e-01 1.000000000000000000e+00 2.449917824165481761e-09 4.793806410857692768e-13 9.827108979225158691e-01 1.000000000000000000e+00 9.518706798553466797e-01 4.392772812167322627e-09 6.113789975643157959e-03 4.152511974098160863e-05 4.196180736215637808e-09 7.207927703857421875e-01 2.705646342008542066e-14 6.214135623849870171e-07 9.999998807907104492e-01 9.499107003211975098e-01 1.000000000000000000e+00 1.000000000000000000e+00 1.000000000000000000e+00 1.000000000000000000e+00 1.000000000000000000e+00 9.999998807907104492e-01 1.134839401270570924e-12 1.000000000000000000e+00 1.000000000000000000e+00 1.772474402327793816e-22 9.627295136451721191e-01 8.082498652584035881e-07 5.288467742502689362e-03 1.000000000000000000e+00 1.356615761025602163e-14 9.699743986129760742e-01 9.680391289293766022e-03 1.000000000000000000e+00 3.494189800782449007e-13 1.000000000000000000e+00 3.159084932123808198e-14 2.154111511019039804e-14 5.770184313065346467e-16 1.000000000000000000e+00 1.002021781459916383e-05 9.999998807907104492e-01 8.955678204074501991e-04 1.000000000000000000e+00 9.489459000600186244e-18 8.299213051795959473e-01 9.961280226707458496e-01 9.470678567886352539e-01 1.103274103880202014e-22 1.000000000000000000e+00 6.979074478149414062e-01 2.365609405194221800e-20 1.000000000000000000e+00 1.000000000000000000e+00 1.236146737271584528e-13 6.457178387790918350e-04 5.910291671752929688e-01 9.847130749696120233e-11 1.000000000000000000e+00 2.832969698829401750e-07 3.806088219523060032e-21 4.788258164282160009e-21 1.000000000000000000e+00 1.000000000000000000e+00 9.999659061431884766e-01 6.373043248686371953e-08 9.844582080841064453e-01 1.429801388397322626e-09 9.504914879798889160e-01 9.991403818130493164e-01 2.418865845658057272e-19 1.000000000000000000e+00 2.270782504153226976e-17 2.376812939172689987e-12 1.000000000000000000e+00 1.241249365389798104e-14 1.346701979637145996e-01 3.604641086571485015e-16 3.174040572003981712e-17 2.682143889551155425e-18 1.000000000000000000e+00 1.000000000000000000e+00 1.364883929491043091e-01 4.823155208555363060e-09 8.947684168815612793e-01 4.979012906551361084e-02 9.936627149581909180e-01 1.000000000000000000e+00 6.171471613924950361e-05 1.000000000000000000e+00 3.350817401326366962e-10 9.962311387062072754e-01 8.754302263259887695e-01 1.577300601240949618e-08 1.000000000000000000e+00 8.513422443141155371e-14 1.534198522347082760e-13 4.049778076177301201e-16 5.455599006151120746e-18 8.422639439231716096e-06 6.625648587942123413e-02 2.438588886377601739e-09 1.000000000000000000e+00 1.000000000000000000e+00 3.147949101389713178e-08 7.443545779750593283e-11 7.562025007915029740e-13 9.984059929847717285e-01 1.000000000000000000e+00 1.000000000000000000e+00 9.997273981571197510e-02 6.106127430939578549e-13 4.462333163246512413e-05 9.999997615814208984e-01 1.432137628991099035e-24 9.999928474426269531e-01 1.000000000000000000e+00 2.727753134479371511e-09 1.000000000000000000e+00 2.289682043965513003e-07 9.587925076484680176e-01 9.999778270721435547e-01 1.000000000000000000e+00 1.434007310308516026e-03 7.365300120909523685e-07

